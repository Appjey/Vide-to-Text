{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:32:08.168613Z",
     "start_time": "2024-12-08T16:31:35.685658Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install ipywidgets widgetsnbextension pandas-profiling",
   "id": "d6a86009494ce96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: widgetsnbextension in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (4.0.13)\n",
      "Collecting pandas-profiling\n",
      "  Using cached pandas_profiling-3.2.0-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipywidgets) (8.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting joblib~=1.1.0 (from pandas-profiling)\n",
      "  Using cached joblib-1.1.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (1.14.1)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (2.2.2)\n",
      "Collecting matplotlib>=3.2.0 (from pandas-profiling)\n",
      "  Using cached matplotlib-3.9.3-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pydantic>=1.8.1 (from pandas-profiling)\n",
      "  Using cached pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (6.0.2)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (2.1.5)\n",
      "Collecting visions==0.7.4 (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Using cached visions-0.7.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.16.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (1.26.3)\n",
      "Requirement already satisfied: htmlmin>=0.1.12 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (0.1.12)\n",
      "Collecting missingno>=0.4.2 (from pandas-profiling)\n",
      "  Using cached missingno-0.5.2-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting phik>=0.11.1 (from pandas-profiling)\n",
      "  Using cached phik-0.12.4-cp312-cp312-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting tangled-up-in-unicode==0.2.0 (from pandas-profiling)\n",
      "  Using cached tangled_up_in_unicode-0.2.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests>=2.24.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas-profiling) (4.66.5)\n",
      "Collecting seaborn>=0.10.1 (from pandas-profiling)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting multimethod>=1.4 (from pandas-profiling)\n",
      "  Using cached multimethod-1.12-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (24.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from visions==0.7.4->visions[type_image_path]==0.7.4->pandas-profiling) (3.2.1)\n",
      "Collecting imagehash (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Using cached ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: Pillow in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (10.2.0)\n",
      "Requirement already satisfied: colorama in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack_data in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Using cached fonttools-4.55.2-cp312-cp312-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (24.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.2.0->pandas-profiling)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2024.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=1.8.1->pandas-profiling)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic>=1.8.1->pandas-profiling)\n",
      "  Using cached pydantic_core-2.27.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from pydantic>=1.8.1->pandas-profiling) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from requests>=2.24.0->pandas-profiling) (2024.8.30)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling) (1.16.0)\n",
      "Collecting PyWavelets (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Using cached pywavelets-1.8.0-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\projects\\vide-to-text\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Using cached pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
      "Using cached tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "Using cached visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "Using cached joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Using cached matplotlib-3.9.3-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Using cached multimethod-1.12-py3-none-any.whl (10 kB)\n",
      "Using cached phik-0.12.4-cp312-cp312-win_amd64.whl (666 kB)\n",
      "Using cached pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-none-win_amd64.whl (2.0 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.55.2-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "Using cached pywavelets-1.8.0-cp312-cp312-win_amd64.whl (4.2 MB)\n",
      "Installing collected packages: tangled-up-in-unicode, PyWavelets, pyparsing, pydantic-core, multimethod, kiwisolver, jupyterlab-widgets, joblib, fonttools, cycler, contourpy, annotated-types, pydantic, matplotlib, imagehash, visions, seaborn, phik, ipywidgets, missingno, pandas-profiling\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.1\n",
      "    Uninstalling joblib-1.1.1:\n",
      "      Successfully uninstalled joblib-1.1.1\n",
      "Successfully installed PyWavelets-1.8.0 annotated-types-0.7.0 contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.2 imagehash-4.3.1 ipywidgets-8.1.5 joblib-1.1.1 jupyterlab-widgets-3.0.13 kiwisolver-1.4.7 matplotlib-3.9.3 missingno-0.5.2 multimethod-1.12 pandas-profiling-3.2.0 phik-0.12.4 pydantic-2.10.3 pydantic-core-2.27.1 pyparsing-3.2.0 seaborn-0.13.2 tangled-up-in-unicode-0.2.0 visions-0.7.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-learn 1.5.2 requires joblib>=1.2.0, but you have joblib 1.1.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:32:51.645645Z",
     "start_time": "2024-12-08T16:32:51.642319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] ='D:/SDK/HF_cache'"
   ],
   "id": "688e83b1ec830e3f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T16:39:03.873764Z",
     "start_time": "2024-12-08T16:38:51.146116Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "dataset = load_dataset(\"bond005/sberdevices_golos_10h_crowd\")\n",
    "\n",
    "# –ü—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n",
    "print(dataset)\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
    "print(dataset['train'][0])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 7993\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 793\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'transcription'],\n",
      "        num_rows: 9994\n",
      "    })\n",
      "})\n",
      "{'audio': {'path': None, 'array': array([ 3.05175781e-05,  3.05175781e-05,  0.00000000e+00, ...,\n",
      "       -1.09863281e-03, -7.93457031e-04, -1.52587891e-04]), 'sampling_rate': 16000}, 'transcription': '—à–µ—Å—Ç–Ω–∞–¥—Ü–∞—Ç–∞—è —á–∞—Å—Ç—å —Å–µ–∑–æ–Ω–∞ –ø—è—Ç—å —Å–µ—Ä–∏–∞–ª–∞ –ª–µ–º–æ–Ω–∏ —Å–Ω–∏–∫–µ—Ç —Ç—Ä–∏–¥—Ü–∞—Ç—å —Ç—Ä–∏ –Ω–µ—Å—á–∞—Å—Ç—å—è'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "–®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
   "id": "73a01a42c90b422b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:03:30.217226Z",
     "start_time": "2024-12-08T17:00:29.052961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperProcessor\n",
    "import torch\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–∞\n",
    "def preprocess(batch):\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—É–¥–∏–æ\n",
    "    audio = batch[\"audio\"][\"array\"]\n",
    "    input_features = processor.feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç–æ–π\n",
    "    transcription = batch[\"transcription\"]\n",
    "    if transcription is None or transcription.strip() == \"\":\n",
    "        transcription = \"<unk>\"  # –ó–∞–º–µ–Ω—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –Ω–∞ –º–∞—Ä–∫–µ—Ä \"<unk>\"\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "    labels = processor.tokenizer(text=transcription, return_tensors=\"pt\").input_ids[0]\n",
    "\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –±–∞—Ç—á\n",
    "    batch[\"input_features\"] = input_features\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∫ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "processed_dataset = dataset.map(preprocess, remove_columns=[\"audio\", \"transcription\"])\n"
   ],
   "id": "22304a3f7f43686b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7993/7993 [01:06<00:00, 119.92 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 793/793 [00:06<00:00, 127.09 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9994/9994 [01:45<00:00, 94.70 examples/s] \n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–®–∞–≥ 4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏. –í–æ—Ç –ø—Ä–∏–º–µ—Ä –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:"
   ],
   "id": "cf998c6f7a8fb39a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:06:20.355559Z",
     "start_time": "2024-12-08T17:06:19.747173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper_golos\",\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",  # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –≤ WandB\n",
    "    fp16=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 16-–±–∏—Ç–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    ")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    "    tokenizer=processor.tokenizer,  # –£–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    ")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.train()\n"
   ],
   "id": "984ef981985ca30f",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m WhisperForConditionalGeneration\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai/whisper-small\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./whisper_golos\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msteps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_total_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreport_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnone\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –≤ WandB\u001B[39;49;00m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfp16\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 16-–±–∏—Ç–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\u001B[39;49;00m\n\u001B[0;32m     19\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞\u001B[39;00m\n\u001B[0;32m     22\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     23\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     24\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     27\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mprocessor\u001B[38;5;241m.\u001B[39mtokenizer,  \u001B[38;5;66;03m# –£–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏\u001B[39;00m\n\u001B[0;32m     28\u001B[0m )\n",
      "File \u001B[1;32m<string>:131\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, eval_use_gather_object)\u001B[0m\n",
      "File \u001B[1;32mD:\\Projects\\Vide-to-Text\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1730\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1728\u001B[0m \u001B[38;5;66;03m# Initialize device before we proceed\u001B[39;00m\n\u001B[0;32m   1729\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_torch_available():\n\u001B[1;32m-> 1730\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[0;32m   1732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtorchdynamo \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1733\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1734\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ü§ó Transformers. Use\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1735\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `torch_compile_backend` instead\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1736\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m   1737\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Projects\\Vide-to-Text\\.venv\\Lib\\site-packages\\transformers\\training_args.py:2227\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2223\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2224\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[0;32m   2225\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2226\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m-> 2227\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[1;32mD:\\Projects\\Vide-to-Text\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:60\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[1;34m(self, obj, objtype)\u001B[0m\n\u001B[0;32m     58\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 60\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[1;32mD:\\Projects\\Vide-to-Text\\.venv\\Lib\\site-packages\\transformers\\training_args.py:2103\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[0;32m   2102\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[1;32m-> 2103\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m   2104\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2105\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2106\u001B[0m         )\n\u001B[0;32m   2107\u001B[0m \u001B[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001B[39;00m\n\u001B[0;32m   2108\u001B[0m accelerator_state_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menabled\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_configured_state\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m}\n",
      "\u001B[1;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–®–∞–≥ 5: –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –≤—ã –º–æ–∂–µ—Ç–µ –æ—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:"
   ],
   "id": "330e1b9b863817e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T16:41:48.863281Z",
     "start_time": "2024-12-08T16:41:48.837033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "trainer.evaluate(processed_dataset[\"test\"], metric_key_prefix=\"test\")\n"
   ],
   "id": "ffa3a1c16f09fe23",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (D:\\Projects\\Vide-to-Text\\.venv\\Lib\\site-packages\\datasets\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_metric\n\u001B[0;32m      3\u001B[0m wer_metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_metrics\u001B[39m(pred):\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'load_metric' from 'datasets' (D:\\Projects\\Vide-to-Text\\.venv\\Lib\\site-packages\\datasets\\__init__.py)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "\n",
    "–ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:"
   ],
   "id": "bfe71107bc8d3b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save_pretrained(\"./whisper_golos_finetuned\")\n",
    "processor.save_pretrained(\"./whisper_golos_finetuned\")\n"
   ],
   "id": "311ad52f44cb1302"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
